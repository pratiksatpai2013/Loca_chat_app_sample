# Local_chat_app_sample

# ðŸ§  Ollama LLM Chat API with FastAPI (Google Colab Edition)

This project demonstrates how to pull an LLM model using **[Ollama](https://ollama.com/)** and serve it as a **chat API** using **FastAPI**, all inside a **Google Colab** environment.

### ðŸ“¦ Tech Stack
- ðŸ¦™ Ollama (for running local LLMs)
- âš¡ FastAPI (for exposing the chat interface as an API)
- ðŸš€ Uvicorn (ASGI server to run FastAPI)
- ðŸ§ª Google Colab (as the environment to run everything)
- ðŸ§  Langchain, Langgraph

---

## ðŸ’¡ Features

âœ… Pull and run an LLM model (like `llama2`, `mistral`, or `phi`) using Ollama  
âœ… Expose the model via a simple FastAPI endpoint  
âœ… Send chat messages and get model-generated responses  
âœ… Designed to run entirely on **Google Colab** for quick testing  
